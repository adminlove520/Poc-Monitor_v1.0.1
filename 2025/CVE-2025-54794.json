[{"id":1033055362,"node_id":"R_kgDOPZMsgg","name":"CVE-2025-54794-Hijacking-Claude-AI-with-a-Prompt-Injection-The-Jailbreak-That-Talked-Back","full_name":"AdityaBhatt3010/CVE-2025-54794-Hijacking-Claude-AI-with-a-Prompt-Injection-The-Jailbreak-That-Talked-Back","private":false,"owner":{"login":"AdityaBhatt3010","id":96762636,"node_id":"U_kgDOBcR7DA","avatar_url":"https://avatars.githubusercontent.com/u/96762636?v=4","url":"https://api.github.com/users/AdityaBhatt3010","html_url":"https://github.com/AdityaBhatt3010","type":"User","site_admin":false},"html_url":"https://github.com/AdityaBhatt3010/CVE-2025-54794-Hijacking-Claude-AI-with-a-Prompt-Injection-The-Jailbreak-That-Talked-Back","description":"A high-severity prompt injection flaw in Claude AI proves that even the smartest language models can be turned into weapons â€” all with a few lines of code.","fork":false,"url":"https://api.github.com/repos/AdityaBhatt3010/CVE-2025-54794-Hijacking-Claude-AI-with-a-Prompt-Injection-The-Jailbreak-That-Talked-Back","created_at":"2025-08-06T08:29:35Z","updated_at":"2025-08-06T08:43:32Z","pushed_at":"2025-08-06T08:43:29Z","stargazers_count":1,"watchers_count":1,"forks_count":0,"archived":false,"disabled":false,"allow_forking":true,"is_template":false,"topics":[],"visibility":"public","forks":0,"watchers":1,"default_branch":"main","score":1}]